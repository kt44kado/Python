# Notion å¯¾è©±å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€€æé€ å¯¾ç­–ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 

import os
import json
import jsonschema
import threading

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

import streamlit as st
import asyncio

from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from typing import Any, Optional

class McpNotionClient:
    def __init__(self, notion_api_key: str):
        self.notion_api_key = notion_api_key
        self._thread = None
        self._loop = None
        self._ready = threading.Event()
        self._shutdown = None
        self._session = None
        self.tools = []

        self.server_params = StdioServerParameters(
            command="npx",
            args=["-y", "@notionhq/notion-mcp-server"],
            env={**os.environ, 
                 "NOTION_API_KEY": self.notion_api_key
                },
        )

    def start(self):
        self._thread = threading.Thread(target=self._thread_main, daemon=True)
        self._thread.start()
        self._ready.wait(timeout=60)
        if not self._ready.is_set():
            raise RuntimeError("MCP Notion client failed to become ready in time.")

    def close(self):
        if not self._loop:
            return
        asyncio.run_coroutine_threadsafe(self._async_shutdown(), self._loop).result(timeout=30)
        self._loop.call_soon_threadsafe(self._loop.stop)
        if self._thread:
            self._thread.join(timeout=30)

    def call_tool(self, tool_name: str, arguments: dict):
        if not self._ready.is_set():
            raise RuntimeError("MCP Notion client is not ready yet.")
        coro = self._session.call_tool(name=tool_name, arguments=arguments)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result(timeout=60)

    def _thread_main(self):
        self._loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self._loop)
        self._loop.create_task(self._runner())
        self._loop.run_forever()

    async def _runner(self):
        self._shutdown = asyncio.Event()
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                self._session = session
                resp = await session.list_tools()
                self.tools = resp.tools
                self._ready.set()
                await self._shutdown.wait()

    async def _async_shutdown(self):
        if self._shutdown:
            self._shutdown.set()


def format_tools_for_prompt(mcp_tools) -> str:
    lines = []
    for t in mcp_tools:
        lines.append(
            f"- name: {t.name}\n"
            f"  description: {t.description}\n"
            f"  inputSchema: {json.dumps(t.inputSchema, ensure_ascii=False)}\n"
        )
    return "\n".join(lines)

def _json_dump_if_needed(x):
    # dict/list ã‚’ JSONæ–‡å­—åˆ—ã«
    if isinstance(x, (dict, list)):
        return json.dumps(x, ensure_ascii=False)
    return x

def normalize_mcp_args(tool_name: str, args: dict) -> dict:
    """
    notion-mcp-server ã®ã‚¹ã‚­ãƒ¼ãƒéƒ½åˆã§
    icon/cover ãŒ "string(format: json)"ã€children ãŒ "array of string" ã«ãªã£ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã‚’å¸åã€‚
    """
    if not isinstance(args, dict):
        return args

    # API-post-page ã®å…¸å‹ï¼šicon/cover ã¯ JSON æ–‡å­—åˆ—ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹
    if tool_name == "API-post-page":
        if "icon" in args:
            args["icon"] = _json_dump_if_needed(args["icon"])
        if "cover" in args:
            args["cover"] = _json_dump_if_needed(args["cover"])

        # children: schemaã ã¨ items ãŒ string ãªã®ã§ã€dict ã®ã¾ã¾æ¸¡ã™ã¨å¼¾ã‹ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
        if "children" in args and isinstance(args["children"], list):
            args["children"] = [_json_dump_if_needed(b) for b in args["children"]]

    return args

def make_mcp_function_tool(mcp_client, mcp_tool_def):
    input_schema = mcp_tool_def.inputSchema

    def _call(arguments: Optional[dict] = None, **kwargs: Any) -> dict:
        # LLMãŒ arguments={...} ã§æ¸¡ã—ã¦ãã¦ã‚‚ã€parent=... ã§æ¸¡ã—ã¦ãã¦ã‚‚å¸å
        payload = {}
        if isinstance(arguments, dict):
            payload.update(arguments)
        payload.update(kwargs)

        # ä»»æ„ï¼šå‰ã«å…¥ã‚ŒãŸæ­£è¦åŒ–ï¼ˆchildren/icon/cover ç­‰ï¼‰ã‚’ä½¿ã†ãªã‚‰ã“ã“ã§
        # payload = normalize_mcp_args(mcp_tool_def.name, payload)

        # å…¥åŠ›æ¤œè¨¼ï¼ˆå…¥ã‚Œã¦ã„ã‚‹ãªã‚‰ï¼‰
        jsonschema.validate(instance=payload, schema=input_schema)

        result = mcp_client.call_tool(mcp_tool_def.name, payload)

        try:
            return result.model_dump()
        except Exception:
            if hasattr(result, "content"):
                return {"content": result.content, "isError": getattr(result, "isError", False)}
            return {"result": result}

    return FunctionTool(
        _call,
        name=mcp_tool_def.name,          # ä¾‹: "API-post-page"
        description=mcp_tool_def.description,
    )

st.set_page_config(page_title="AutoGen x Streamlit App", layout="centered")
st.title("ğŸ¤– AutoGen å¯¾è©±å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

# --- 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰ ---
@st.cache_resource
def get_mcp_client():
    notion_token = st.secrets["NOTION_TOKEN"] # Streamlitã§ã¯Secretsç®¡ç†ã‚’æ¨å¥¨
    client = McpNotionClient(notion_api_key=notion_token)
    client.start()  # æ¥ç¶šé–‹å§‹
    return client

# --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡2: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«ã®æ§‹ç¯‰ ---
@st.cache_resource
def get_assistant():
    mcp_client = get_mcp_client()

    # system_message ãŒå£Šã‚Œã¦ã„ãŸã®ã§ """ """ ã§ç¢ºå®Ÿã«å…¥ã‚Œã‚‹
    tools_catalog = format_tools_for_prompt(mcp_client.tools)
    system_message = f"""You are an assistant that manipulates Notion via MCP tools.

RULES:
- To perform any Notion action, you MUST call one of the provided tools.
- NEVER invent tool names. Only call the tools provided to you.
- For each tool call, pass arguments that match the tool's inputSchema.

MCP tool catalog (reference):
{tools_catalog}
"""

    model_client = OpenAIChatCompletionClient(model="gpt-5-mini")

    notion_tools = [make_mcp_function_tool(mcp_client, t) for t in mcp_client.tools]

    assistant = AssistantAgent(
        name="assistant",
        system_message=system_message,
        model_client=model_client,
        tools=notion_tools,   # â† ã“ã“ãŒé‡è¦ï¼šmcp_call_tool 1å€‹ã§ã¯ãªãå…¨ãƒ„ãƒ¼ãƒ«ã‚’åˆ—æŒ™
    )
    return assistant

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
# --- 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰ ---
assistant = get_assistant()

# --- 2. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆä¼šè©±å±¥æ­´ã®ä¿å­˜ç”¨ï¼‰ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 3. ä¿å­˜ã•ã‚ŒãŸå±¥æ­´ã®è¡¨ç¤º ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆéåŒæœŸé–¢æ•°ã¨ã—ã¦å®šç¾©ï¼‰ ---
async def run_chat(prompt):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ç”»é¢ã«è¡¨ç¤º & å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”é ˜åŸŸã‚’ä½œæˆ
    with st.chat_message("assistant"):
        container = st.empty()  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºç”¨ã®ç©ºæ 
        full_response = ""
        
        # run_stream ã‚’ä½¿ç”¨ã—ã¦é€æ¬¡å–å¾—
        # â€» å®Ÿéš›ã®å®Ÿè£…ã§ã¯ TaskResult ãŒæµã‚Œã¦ãã‚‹ãŸã‚ã€ãã‚Œã‚’å–ã‚Šå‡ºã™
        async for chunk in assistant.run_stream(task=prompt):
            # 1. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå›ç­”ã®å‡¦ç†
            if isinstance(chunk, TextMessage) and chunk.source == assistant.name:
                full_response += chunk.content
                container.markdown(full_response + "â–Œ") # ã‚«ãƒ¼ã‚½ãƒ«é¢¨ã®æ¼”å‡º
            
            # 2. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã®å‡¦ç†ã‚’è¿½åŠ 
            elif hasattr(chunk, 'is_error') and chunk.is_error:
                error_msg = f"\n\nâš ï¸ **ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼:** {chunk.content}"
                full_response += error_msg
                container.markdown(full_response)
        
        # æœ€çµ‚çµæœã®è¡¨ç¤ºï¼ˆä½•ã‚‚è¿”ã£ã¦ã“ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not full_response:
            full_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚"
        
        container.markdown(full_response) # æœ€çµ‚çµæœã‚’ç¢ºå®šè¡¨ç¤º
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- 5. å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ---
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    # Streamlitã®åŒæœŸå‡¦ç†ã®ä¸­ã§éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹
    asyncio.run(run_chat(prompt))
